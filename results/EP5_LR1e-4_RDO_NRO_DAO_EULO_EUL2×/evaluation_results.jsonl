{"forget-set": {"overall-regurgitation-score": 0.3427362149893228, "overall-knowledge-score": 0.19653179190751446, "Task1": {"regurgitation-score": 0.45342848929061325, "knowledge-score": 0.16666666666666666}, "Task2": {"regurgitation-score": 0.17885408758168028, "knowledge-score": 0.17391304347826086}, "Task3": {"regurgitation-score": 0.37546193108181714, "knowledge-score": 0.29411764705882354}}, "retain-set": {"overall-regurgitation-score": 0.3458291642339615, "overall-knowledge-score": 0.20105820105820105, "Task1": {"regurgitation-score": 0.4784608792399311, "knowledge-score": 0.2222222222222222}, "Task2": {"regurgitation-score": 0.1547451185618739, "knowledge-score": 0.176}, "Task3": {"regurgitation-score": 0.37815470035939425, "knowledge-score": 0.2702702702702703}}, "mia_loss_acc": 0.982192, "mmlu_average": 0.27496083179034325, "aggregated-terms": [0.5465715107093867, 0.8333333333333334, 0.8211459124183197, 0.8260869565217391, 0.6245380689181829, 0.7058823529411764, 0.4784608792399311, 0.2222222222222222, 0.1547451185618739, 0.176, 0.37815470035939425, 0.2702702702702703], "aggregate-score": 0.22273541701898666, "harmonic-mean-task-aggregate": 0.3576294192666167, "mia_final_score": 0.03561600000000009}