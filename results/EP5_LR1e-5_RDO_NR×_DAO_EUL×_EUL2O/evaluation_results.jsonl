{"forget-set": {"overall-regurgitation-score": 0.7547274169652856, "overall-knowledge-score": 0.9826589595375722, "Task1": {"regurgitation-score": 0.8772598105317643, "knowledge-score": 0.9583333333333334}, "Task2": {"regurgitation-score": 0.8333520907433952, "knowledge-score": 0.991304347826087}, "Task3": {"regurgitation-score": 0.6150466833625793, "knowledge-score": 0.9705882352941176}}, "retain-set": {"overall-regurgitation-score": 0.7245528354267499, "overall-knowledge-score": 0.9894179894179894, "Task1": {"regurgitation-score": 0.9260386088007408, "knowledge-score": 1.0}, "Task2": {"regurgitation-score": 0.7852772960008632, "knowledge-score": 1.0}, "Task3": {"regurgitation-score": 0.5364926355497067, "knowledge-score": 0.9459459459459459}}, "mia_loss_acc": 1.0, "mmlu_average": 0.27496083179034325, "aggregated-terms": [0.12274018946823573, 0.04166666666666663, 0.16664790925660478, 0.008695652173912993, 0.3849533166374207, 0.02941176470588236, 0.9260386088007408, 1.0, 0.7852772960008632, 1.0, 0.5364926355497067, 0.9459459459459459], "aggregate-score": 0.11195610878147212, "harmonic-mean-task-aggregate": 0.06090749455407311, "mia_final_score": 0.0}