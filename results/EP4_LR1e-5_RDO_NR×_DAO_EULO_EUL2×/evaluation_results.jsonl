{"forget-set": {"overall-regurgitation-score": 0.7468510403093921, "overall-knowledge-score": 0.9826589595375722, "Task1": {"regurgitation-score": 0.8404432653681795, "knowledge-score": 0.9583333333333334}, "Task2": {"regurgitation-score": 0.8587653731131992, "knowledge-score": 0.991304347826087}, "Task3": {"regurgitation-score": 0.6050791857241429, "knowledge-score": 0.9705882352941176}}, "retain-set": {"overall-regurgitation-score": 0.7083397691423649, "overall-knowledge-score": 0.9947089947089947, "Task1": {"regurgitation-score": 0.9204019197178006, "knowledge-score": 1.0}, "Task2": {"regurgitation-score": 0.7517968664334579, "knowledge-score": 1.0}, "Task3": {"regurgitation-score": 0.5242288097419843, "knowledge-score": 0.972972972972973}}, "mia_loss_acc": 0.9999359999999999, "mmlu_average": 0.27496083179034325, "aggregated-terms": [0.15955673463182052, 0.04166666666666663, 0.14123462688680077, 0.008695652173912993, 0.3949208142758571, 0.02941176470588236, 0.9204019197178006, 1.0, 0.7517968664334579, 1.0, 0.5242288097419843, 0.972972972972973], "aggregate-score": 0.11208032254770021, "harmonic-mean-task-aggregate": 0.06115213585275728, "mia_final_score": 0.000128000000000128}