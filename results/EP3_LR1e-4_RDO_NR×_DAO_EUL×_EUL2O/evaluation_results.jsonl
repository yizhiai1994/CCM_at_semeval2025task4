{"forget-set": {"overall-regurgitation-score": 0.6767638979553502, "overall-knowledge-score": 0.9826589595375722, "Task1": {"regurgitation-score": 0.8041022318958605, "knowledge-score": 1.0}, "Task2": {"regurgitation-score": 0.8491052337079322, "knowledge-score": 0.9826086956521739}, "Task3": {"regurgitation-score": 0.4702941704000077, "knowledge-score": 0.9705882352941176}}, "retain-set": {"overall-regurgitation-score": 0.6011204978602063, "overall-knowledge-score": 0.9841269841269841, "Task1": {"regurgitation-score": 0.9090539459977725, "knowledge-score": 1.0}, "Task2": {"regurgitation-score": 0.6569982261094824, "knowledge-score": 0.976}, "Task3": {"regurgitation-score": 0.33865708418598534, "knowledge-score": 1.0}}, "mia_loss_acc": 0.974256, "mmlu_average": 0.27496083179034325, "aggregated-terms": [0.1958977681041395, 0.0, 0.15089476629206777, 0.017391304347826098, 0.5297058295999924, 0.02941176470588236, 0.9090539459977725, 1.0, 0.6569982261094824, 0.976, 0.33865708418598534, 1.0], "aggregate-score": 0.10881627726344774, "harmonic-mean-task-aggregate": 0, "mia_final_score": 0.05148799999999998}