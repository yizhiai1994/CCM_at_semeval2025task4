{"forget-set": {"overall-regurgitation-score": 0.9010596333129411, "overall-knowledge-score": 0.24277456647398843, "Task1": {"regurgitation-score": 0.9126984126984127, "knowledge-score": 0.4583333333333333}, "Task2": {"regurgitation-score": 0.7932302352379077, "knowledge-score": 0.11304347826086956}, "Task3": {"regurgitation-score": 0.9657874406798366, "knowledge-score": 0.5294117647058824}}, "retain-set": {"overall-regurgitation-score": 0.894979325994014, "overall-knowledge-score": 0.21693121693121692, "Task1": {"regurgitation-score": 0.9517268488000196, "knowledge-score": 0.6296296296296297}, "Task2": {"regurgitation-score": 0.7284468798115022, "knowledge-score": 0.088}, "Task3": {"regurgitation-score": 0.9660908946102477, "knowledge-score": 0.35135135135135137}}, "mia_loss_acc": 1.0, "mmlu_average": 0.2695484973650477, "aggregated-terms": [0.08730158730158732, 0.5416666666666667, 0.2067697647620923, 0.8869565217391304, 0.03421255932016343, 0.47058823529411764, 0.9517268488000196, 0.6296296296296297, 0.7284468798115022, 0.088, 0.9660908946102477, 0.35135135135135137], "aggregate-score": 0.14709448974393824, "harmonic-mean-task-aggregate": 0.17173497186676698, "mia_final_score": 0.0}