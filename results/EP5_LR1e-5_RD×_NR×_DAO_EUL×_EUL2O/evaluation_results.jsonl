{"forget-set": {"overall-regurgitation-score": 0.7596114493742595, "overall-knowledge-score": 0.9653179190751445, "Task1": {"regurgitation-score": 0.6951393401314, "knowledge-score": 0.875}, "Task2": {"regurgitation-score": 0.8548723036223036, "knowledge-score": 0.9826086956521739}, "Task3": {"regurgitation-score": 0.7406800074367194, "knowledge-score": 0.9705882352941176}}, "retain-set": {"overall-regurgitation-score": 0.7675096501710654, "overall-knowledge-score": 0.9576719576719577, "Task1": {"regurgitation-score": 0.7958630341545385, "knowledge-score": 0.9259259259259259}, "Task2": {"regurgitation-score": 0.875461088912789, "knowledge-score": 0.96}, "Task3": {"regurgitation-score": 0.6738791816279066, "knowledge-score": 0.972972972972973}}, "mia_loss_acc": 1.0, "mmlu_average": 0.27496083179034325, "aggregated-terms": [0.3048606598686, 0.125, 0.1451276963776964, 0.017391304347826098, 0.25931999256328064, 0.02941176470588236, 0.7958630341545385, 0.9259259259259259, 0.875461088912789, 0.96, 0.6738791816279066, 0.972972972972973], "aggregate-score": 0.12483237157043757, "harmonic-mean-task-aggregate": 0.09953628292096944, "mia_final_score": 0.0}