{"forget-set": {"overall-regurgitation-score": 0.18001746205878272, "overall-knowledge-score": 0.49710982658959535, "Task1": {"regurgitation-score": 0.12215463635487918, "knowledge-score": 0.375}, "Task2": {"regurgitation-score": 0.2714422754366082, "knowledge-score": 0.6}, "Task3": {"regurgitation-score": 0.15901561232947975, "knowledge-score": 0.23529411764705882}}, "retain-set": {"overall-regurgitation-score": 0.1622338349673176, "overall-knowledge-score": 0.5185185185185185, "Task1": {"regurgitation-score": 0.16196848292266905, "knowledge-score": 0.25925925925925924}, "Task2": {"regurgitation-score": 0.23602651959596635, "knowledge-score": 0.672}, "Task3": {"regurgitation-score": 0.11256754819675802, "knowledge-score": 0.1891891891891892}}, "mia_loss_acc": 0.9988, "mmlu_average": 0.2796610169491525, "aggregated-terms": [0.8778453636451208, 0.625, 0.7285577245633919, 0.4, 0.8409843876705203, 0.7647058823529411, 0.16196848292266905, 0.25925925925925924, 0.23602651959596635, 0.672, 0.11256754819675802, 0.1891891891891892], "aggregate-score": 0.19649546504282336, "harmonic-mean-task-aggregate": 0.30742537817931764, "mia_final_score": 0.0023999999999999577}