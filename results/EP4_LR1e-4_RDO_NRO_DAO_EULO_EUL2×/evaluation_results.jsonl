{"forget-set": {"overall-regurgitation-score": 0.5094896645898848, "overall-knowledge-score": 0.2138728323699422, "Task1": {"regurgitation-score": 0.738350070375466, "knowledge-score": 0.3333333333333333}, "Task2": {"regurgitation-score": 0.3097041492630831, "knowledge-score": 0.1565217391304348}, "Task3": {"regurgitation-score": 0.48309016793289955, "knowledge-score": 0.3235294117647059}}, "retain-set": {"overall-regurgitation-score": 0.48912108306501245, "overall-knowledge-score": 0.19576719576719576, "Task1": {"regurgitation-score": 0.7017441215159317, "knowledge-score": 0.4074074074074074}, "Task2": {"regurgitation-score": 0.26849705306962884, "knowledge-score": 0.144}, "Task3": {"regurgitation-score": 0.4830340212193305, "knowledge-score": 0.21621621621621623}}, "mia_loss_acc": 0.9859680000000001, "mmlu_average": 0.27496083179034325, "aggregated-terms": [0.26164992962453404, 0.6666666666666667, 0.6902958507369169, 0.8434782608695652, 0.5169098320671004, 0.6764705882352942, 0.7017441215159317, 0.4074074074074074, 0.26849705306962884, 0.144, 0.4830340212193305, 0.21621621621621623], "aggregate-score": 0.22366007053593945, "harmonic-mean-task-aggregate": 0.3679553798174752, "mia_final_score": 0.028063999999999867}