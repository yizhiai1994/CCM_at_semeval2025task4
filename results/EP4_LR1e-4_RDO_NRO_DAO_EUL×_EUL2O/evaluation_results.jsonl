{"forget-set": {"overall-regurgitation-score": 0.5307770648706831, "overall-knowledge-score": 0.20809248554913296, "Task1": {"regurgitation-score": 0.6687819679940862, "knowledge-score": 0.375}, "Task2": {"regurgitation-score": 0.3713314992791366, "knowledge-score": 0.10434782608695652}, "Task3": {"regurgitation-score": 0.5412220746837388, "knowledge-score": 0.4411764705882353}}, "retain-set": {"overall-regurgitation-score": 0.5125891630932645, "overall-knowledge-score": 0.19576719576719576, "Task1": {"regurgitation-score": 0.6669594944475378, "knowledge-score": 0.4444444444444444}, "Task2": {"regurgitation-score": 0.3897852427069385, "knowledge-score": 0.112}, "Task3": {"regurgitation-score": 0.48291616479847516, "knowledge-score": 0.2972972972972973}}, "mia_loss_acc": 0.9894080000000001, "mmlu_average": 0.27496083179034325, "aggregated-terms": [0.3312180320059138, 0.625, 0.6286685007208634, 0.8956521739130435, 0.45877792531626116, 0.5588235294117647, 0.6669594944475378, 0.4444444444444444, 0.3897852427069385, 0.112, 0.48291616479847516, 0.2972972972972973], "aggregate-score": 0.22381924021628327, "harmonic-mean-task-aggregate": 0.37531288885850667, "mia_final_score": 0.02118399999999987}