{"forget-set": {"overall-regurgitation-score": 0.4640750663819791, "overall-knowledge-score": 0.2658959537572254, "Task1": {"regurgitation-score": 0.6115278966684904, "knowledge-score": 0.4166666666666667}, "Task2": {"regurgitation-score": 0.3116420848741101, "knowledge-score": 0.20869565217391303}, "Task3": {"regurgitation-score": 0.4631071442585886, "knowledge-score": 0.35294117647058826}}, "retain-set": {"overall-regurgitation-score": 0.43958966341544864, "overall-knowledge-score": 0.21164021164021163, "Task1": {"regurgitation-score": 0.6735833703533062, "knowledge-score": 0.48148148148148145}, "Task2": {"regurgitation-score": 0.2596099189455649, "knowledge-score": 0.144}, "Task3": {"regurgitation-score": 0.39044543434585266, "knowledge-score": 0.24324324324324326}}, "mia_loss_acc": 0.9891840000000001, "mmlu_average": 0.27496083179034325, "aggregated-terms": [0.3884721033315096, 0.5833333333333333, 0.6883579151258898, 0.7913043478260869, 0.5368928557414114, 0.6470588235294117, 0.6735833703533062, 0.48148148148148145, 0.2596099189455649, 0.144, 0.39044543434585266, 0.24324324324324326], "aggregate-score": 0.22607798773527396, "harmonic-mean-task-aggregate": 0.38164113141547873, "mia_final_score": 0.021631999999999874}