{"forget-set": {"overall-regurgitation-score": 0.7764627536889649, "overall-knowledge-score": 0.9710982658959537, "Task1": {"regurgitation-score": 0.8876764771984309, "knowledge-score": 0.9166666666666666}, "Task2": {"regurgitation-score": 0.8649065476286649, "knowledge-score": 0.9826086956521739}, "Task3": {"regurgitation-score": 0.6381293235466035, "knowledge-score": 0.9705882352941176}}, "retain-set": {"overall-regurgitation-score": 0.712054777272415, "overall-knowledge-score": 0.9894179894179894, "Task1": {"regurgitation-score": 0.8918595362532137, "knowledge-score": 1.0}, "Task2": {"regurgitation-score": 0.7710359578885576, "knowledge-score": 1.0}, "Task3": {"regurgitation-score": 0.5409937500322759, "knowledge-score": 0.9459459459459459}}, "mia_loss_acc": 1.0, "mmlu_average": 0.27496083179034325, "aggregated-terms": [0.1123235228015691, 0.08333333333333337, 0.13509345237133508, 0.017391304347826098, 0.36187067645339654, 0.02941176470588236, 0.8918595362532137, 1.0, 0.7710359578885576, 1.0, 0.5409937500322759, 0.9459459459459459], "aggregate-score": 0.12244833980257644, "harmonic-mean-task-aggregate": 0.09238418761738605, "mia_final_score": 0.0}