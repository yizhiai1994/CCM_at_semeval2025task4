{"forget-set": {"overall-regurgitation-score": 0.8753427102458223, "overall-knowledge-score": 0.14450867052023122, "Task1": {"regurgitation-score": 0.9071924603174604, "knowledge-score": 0.3333333333333333}, "Task2": {"regurgitation-score": 0.7402206441848386, "knowledge-score": 0.017391304347826087}, "Task3": {"regurgitation-score": 0.9442666372365079, "knowledge-score": 0.4411764705882353}}, "retain-set": {"overall-regurgitation-score": 0.8754175594026044, "overall-knowledge-score": 0.12698412698412698, "Task1": {"regurgitation-score": 0.9433636468884304, "knowledge-score": 0.48148148148148145}, "Task2": {"regurgitation-score": 0.6983449219991997, "knowledge-score": 0.0}, "Task3": {"regurgitation-score": 0.9454789532665989, "knowledge-score": 0.2972972972972973}}, "mia_loss_acc": 1.0, "mmlu_average": 0.2695484973650477, "aggregated-terms": [0.09280753968253963, 0.6666666666666667, 0.25977935581516143, 0.9826086956521739, 0.055733362763492056, 0.5588235294117647, 0.9433636468884304, 0.48148148148148145, 0.6983449219991997, 0.0, 0.9454789532665989, 0.2972972972972973], "aggregate-score": 0.08984949912168257, "harmonic-mean-task-aggregate": 0, "mia_final_score": 0.0}