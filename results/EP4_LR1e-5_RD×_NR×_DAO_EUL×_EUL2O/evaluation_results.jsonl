{"forget-set": {"overall-regurgitation-score": 0.7445600561652226, "overall-knowledge-score": 0.9710982658959537, "Task1": {"regurgitation-score": 0.6605567475594428, "knowledge-score": 0.875}, "Task2": {"regurgitation-score": 0.8495928005167136, "knowledge-score": 0.991304347826087}, "Task3": {"regurgitation-score": 0.7328049469432942, "knowledge-score": 0.9705882352941176}}, "retain-set": {"overall-regurgitation-score": 0.7462883904424706, "overall-knowledge-score": 0.9682539682539683, "Task1": {"regurgitation-score": 0.7854887759589296, "knowledge-score": 0.9259259259259259}, "Task2": {"regurgitation-score": 0.8562957023280475, "knowledge-score": 0.976}, "Task3": {"regurgitation-score": 0.6433534389266917, "knowledge-score": 0.972972972972973}}, "mia_loss_acc": 1.0, "mmlu_average": 0.27496083179034325, "aggregated-terms": [0.3394432524405572, 0.125, 0.1504071994832864, 0.008695652173912993, 0.26719505305670577, 0.02941176470588236, 0.7854887759589296, 0.9259259259259259, 0.8562957023280475, 0.976, 0.6433534389266917, 0.972972972972973], "aggregate-score": 0.11419328665689643, "harmonic-mean-task-aggregate": 0.06761902818034606, "mia_final_score": 0.0}