{"forget-set": {"overall-regurgitation-score": 0.5804719222885372, "overall-knowledge-score": 0.9826589595375722, "Task1": {"regurgitation-score": 0.6170721122228312, "knowledge-score": 0.9583333333333334}, "Task2": {"regurgitation-score": 0.7093617325753432, "knowledge-score": 0.991304347826087}, "Task3": {"regurgitation-score": 0.4674463283173734, "knowledge-score": 0.9705882352941176}}, "retain-set": {"overall-regurgitation-score": 0.5516107791320828, "overall-knowledge-score": 0.9417989417989417, "Task1": {"regurgitation-score": 0.5551910757962136, "knowledge-score": 0.9629629629629629}, "Task2": {"regurgitation-score": 0.5948190130622979, "knowledge-score": 0.976}, "Task3": {"regurgitation-score": 0.5198033775594635, "knowledge-score": 0.8108108108108109}}, "mia_loss_acc": 1.0, "mmlu_average": 0.2743198974505056, "aggregated-terms": [0.3829278877771688, 0.04166666666666663, 0.2906382674246568, 0.008695652173912993, 0.5325536716826266, 0.02941176470588236, 0.5551910757962136, 0.9629629629629629, 0.5948190130622979, 0.976, 0.5198033775594635, 0.8108108108108109], "aggregate-score": 0.11253340091065159, "harmonic-mean-task-aggregate": 0.06328030528144915, "mia_final_score": 0.0}